{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1: Imitation Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8y_M1tGxmGhT"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test virtual display. If you see a video of a four-legged ant fumbling about, setup is complete!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y7cywOEgo4a8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from cs285.infrastructure.colab_utils import (\n",
        "    wrap_env,\n",
        "    show_video\n",
        ")\n",
        "\n",
        "env = wrap_env(gym.make(\"Ant-v4\", render_mode='rgb_array'))\n",
        "\n",
        "observation = env.reset()\n",
        "for i in range(100):\n",
        "    env.render()\n",
        "    obs, rew, term, _ = env.step(env.action_space.sample() ) \n",
        "    if term:\n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "print('Loading video...')\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UunygyDXrx7k"
      },
      "source": [
        "## Run Behavior Cloning (Problem 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "enh5ZMHftEO7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
        "from cs285.agents.bc_agent import BCAgent\n",
        "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
        "from cs285.infrastructure.utils import MJ_ENV_KWARGS, MJ_ENV_NAMES"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Runtime arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "imnAkQ6jryL7"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    return getattr(self, key)\n",
        "\n",
        "  def __setitem__(self, key, val):\n",
        "    setattr(self, key, val)\n",
        "\n",
        "  #@markdown expert data\n",
        "  expert_policy_file = 'cs285/policies/experts/Ant.pkl' #@param\n",
        "  expert_data = 'cs285/expert_data/expert_data_Ant-v4.pkl' #@param\n",
        "  env_name = 'Ant-v4' #@param ['Ant-v4', 'Walker2d-v4', 'HalfCheetah-v4', 'Hopper-v4']\n",
        "  exp_name = 'bc_ant' #@param\n",
        "  do_dagger = False #@param {type: \"boolean\"}\n",
        "  ep_len = 1000 #@param {type: \"integer\"}\n",
        "  save_params = False #@param {type: \"boolean\"}\n",
        "\n",
        "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"})\n",
        "  n_iter = 1 #@param {type: \"integer\"})\n",
        "\n",
        "  #@markdown batches & buffers\n",
        "  batch_size = 1000 #@param {type: \"integer\"})\n",
        "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
        "  train_batch_size = 100 #@param {type: \"integer\"}\n",
        "  max_replay_buffer_size = 1000000 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown network\n",
        "  n_layers = 2 #@param {type: \"integer\"}\n",
        "  size = 64 #@param {type: \"integer\"}\n",
        "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
        "\n",
        "  #@markdown logging\n",
        "  video_log_freq = 5 #@param {type: \"integer\"}\n",
        "  scalar_log_freq = 1 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown gpu & run-time settings\n",
        "  no_gpu = False #@param {type: \"boolean\"}\n",
        "  which_gpu = 0 #@param {type: \"integer\"}\n",
        "  seed = 1 #@param {type: \"integer\"}\n",
        "\n",
        "args = Args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fLnU1evmss4I"
      },
      "outputs": [],
      "source": [
        "class BC_Trainer(object):\n",
        "\n",
        "    def __init__(self, params):\n",
        "        #######################\n",
        "        ## AGENT PARAMS\n",
        "        #######################\n",
        "\n",
        "        agent_params = {\n",
        "            'n_layers': params['n_layers'],\n",
        "            'size': params['size'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
        "            }\n",
        "\n",
        "        self.params = params\n",
        "        self.params['agent_class'] = BCAgent ## TODO: look in here and implement this\n",
        "        self.params['agent_params'] = agent_params\n",
        "\n",
        "        self.params[\"env_kwargs\"] = MJ_ENV_KWARGS[self.params['env_name']]\n",
        "\n",
        "        ################\n",
        "        ## RL TRAINER\n",
        "        ################\n",
        "\n",
        "        self.rl_trainer = RL_Trainer(self.params) ## TODO: look in here and implement this\n",
        "\n",
        "        #######################\n",
        "        ## LOAD EXPERT POLICY\n",
        "        #######################\n",
        "\n",
        "        print('Loading expert policy from...', self.params['expert_policy_file'])\n",
        "        self.loaded_expert_policy = LoadedGaussianPolicy(self.params['expert_policy_file'])\n",
        "        print('Done restoring expert policy...')\n",
        "\n",
        "    def run_training_loop(self):\n",
        "\n",
        "        self.rl_trainer.run_training_loop(\n",
        "            n_iter=self.params['n_iter'],\n",
        "            initial_expertdata=self.params['expert_data'],\n",
        "            collect_policy=self.rl_trainer.agent.actor,\n",
        "            eval_policy=self.rl_trainer.agent.actor,\n",
        "            relabel_with_expert=self.params['do_dagger'],\n",
        "            expert_policy=self.loaded_expert_policy,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7UkzHBfxsxH8"
      },
      "outputs": [],
      "source": [
        "#@title create directory for logging\n",
        "\n",
        "if args.do_dagger:\n",
        "    logdir_prefix = 'q2_'  # The autograder uses the prefix `q2_`\n",
        "    assert args.n_iter>1, ('DAgger needs more than 1 iteration (n_iter>1) of training, to iteratively query the expert and train (after 1st warmstarting from behavior cloning).')\n",
        "else:\n",
        "    logdir_prefix = 'q1_'  # The autograder uses the prefix `q1_`\n",
        "    assert args.n_iter==1, ('Vanilla behavior cloning collects expert data just once (n_iter=1)')\n",
        "\n",
        "data_path ='/content/cs285_f2022/hw1/data'\n",
        "if not (os.path.exists(data_path)):\n",
        "    os.makedirs(data_path)\n",
        "logdir = logdir_prefix + args.exp_name + '_' + args.env_name + \\\n",
        "         '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qQb789_syt0"
      },
      "outputs": [],
      "source": [
        "## run training\n",
        "print(args.logdir)\n",
        "trainer = BC_Trainer(args)\n",
        "trainer.run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "75M0MlR5tUIb"
      },
      "outputs": [],
      "source": [
        "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/cs285_f2022/hw1/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9onuUPfPEa"
      },
      "source": [
        "## Running DAgger (Problem 2)\n",
        "Modify the settings above:\n",
        "1. check the `do_dagger` box\n",
        "2. set `n_iters` to `10`\n",
        "3. set `exp_name` to `dagger_ant`\n",
        "and then rerun the code."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of run_hw1.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

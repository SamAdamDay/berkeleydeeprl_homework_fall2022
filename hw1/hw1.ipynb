{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1: Imitation Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Have the notebook reload local modules when they change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cellView": "form",
        "id": "8y_M1tGxmGhT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fc3b0135b10>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test virtual display. If you see a video of a four-legged ant fumbling about, setup is complete!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y7cywOEgo4a8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from cs285.infrastructure.colab_utils import (\n",
        "    wrap_env,\n",
        "    show_video\n",
        ")\n",
        "\n",
        "env = wrap_env(gym.make(\"Ant-v4\", render_mode='rgb_array'))\n",
        "\n",
        "observation = env.reset()\n",
        "for i in range(100):\n",
        "    env.render()\n",
        "    obs, rew, term, _ = env.step(env.action_space.sample() ) \n",
        "    if term:\n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "print('Loading video...')\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UunygyDXrx7k"
      },
      "source": [
        "## Run Behavior Cloning (Problem 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "id": "enh5ZMHftEO7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
        "from cs285.agents.bc_agent import BCAgent\n",
        "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
        "from cs285.infrastructure.utils import MJ_ENV_KWARGS, MJ_ENV_NAMES"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Runtime arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "id": "imnAkQ6jryL7"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    return getattr(self, key)\n",
        "\n",
        "  def __setitem__(self, key, val):\n",
        "    setattr(self, key, val)\n",
        "\n",
        "  #@markdown expert data\n",
        "  expert_policy_file = 'cs285/policies/experts/Ant.pkl' #@param\n",
        "  expert_data = 'cs285/expert_data/expert_data_Ant-v4.pkl' #@param\n",
        "  env_name = 'Ant-v4' #@param ['Ant-v4', 'Walker2d-v4', 'HalfCheetah-v4', 'Hopper-v4']\n",
        "  exp_name = 'dagger_ant' #@param\n",
        "  do_dagger = True #@param {type: \"boolean\"}\n",
        "  ep_len = 1000 #@param {type: \"integer\"}\n",
        "  save_params = False #@param {type: \"boolean\"}\n",
        "\n",
        "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"})\n",
        "  n_iter = 10 #@param {type: \"integer\"})\n",
        "\n",
        "  #@markdown batches & buffers\n",
        "  batch_size = 1000 #@param {type: \"integer\"})\n",
        "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
        "  train_batch_size = 100 #@param {type: \"integer\"}\n",
        "  max_replay_buffer_size = 1000000 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown network\n",
        "  n_layers = 2 #@param {type: \"integer\"}\n",
        "  size = 64 #@param {type: \"integer\"}\n",
        "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
        "\n",
        "  #@markdown logging\n",
        "  video_log_freq = 5 #@param {type: \"integer\"}\n",
        "  scalar_log_freq = 1 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown gpu & run-time settings\n",
        "  no_gpu = False #@param {type: \"boolean\"}\n",
        "  which_gpu = 0 #@param {type: \"integer\"}\n",
        "  seed = 1 #@param {type: \"integer\"}\n",
        "\n",
        "args = Args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "form",
        "id": "fLnU1evmss4I"
      },
      "outputs": [],
      "source": [
        "class BC_Trainer(object):\n",
        "\n",
        "    def __init__(self, params):\n",
        "        #######################\n",
        "        ## AGENT PARAMS\n",
        "        #######################\n",
        "\n",
        "        agent_params = {\n",
        "            'n_layers': params['n_layers'],\n",
        "            'size': params['size'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
        "            }\n",
        "\n",
        "        self.params = params\n",
        "        self.params['agent_class'] = BCAgent ## TODO: look in here and implement this\n",
        "        self.params['agent_params'] = agent_params\n",
        "\n",
        "        self.params[\"env_kwargs\"] = MJ_ENV_KWARGS[self.params['env_name']]\n",
        "\n",
        "        ################\n",
        "        ## RL TRAINER\n",
        "        ################\n",
        "\n",
        "        self.rl_trainer = RL_Trainer(self.params) ## TODO: look in here and implement this\n",
        "\n",
        "        #######################\n",
        "        ## LOAD EXPERT POLICY\n",
        "        #######################\n",
        "\n",
        "        print('Loading expert policy from...', self.params['expert_policy_file'])\n",
        "        self.loaded_expert_policy = LoadedGaussianPolicy(self.params['expert_policy_file'])\n",
        "        print('Done restoring expert policy...')\n",
        "\n",
        "    def run_training_loop(self):\n",
        "\n",
        "        self.rl_trainer.run_training_loop(\n",
        "            n_iter=self.params['n_iter'],\n",
        "            initial_expertdata=self.params['expert_data'],\n",
        "            collect_policy=self.rl_trainer.agent.actor,\n",
        "            eval_policy=self.rl_trainer.agent.actor,\n",
        "            relabel_with_expert=self.params['do_dagger'],\n",
        "            expert_policy=self.loaded_expert_policy,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "id": "7UkzHBfxsxH8"
      },
      "outputs": [],
      "source": [
        "#@title create directory for logging\n",
        "\n",
        "if args.do_dagger:\n",
        "    logdir_prefix = 'q2_'  # The autograder uses the prefix `q2_`\n",
        "    assert args.n_iter>1, ('DAgger needs more than 1 iteration (n_iter>1) of training, to iteratively query the expert and train (after 1st warmstarting from behavior cloning).')\n",
        "else:\n",
        "    logdir_prefix = 'q1_'  # The autograder uses the prefix `q1_`\n",
        "    assert args.n_iter==1, ('Vanilla behavior cloning collects expert data just once (n_iter=1)')\n",
        "\n",
        "data_path ='data'\n",
        "if not (os.path.exists(data_path)):\n",
        "    os.makedirs(data_path)\n",
        "logdir = logdir_prefix + args.exp_name + '_' + args.env_name + \\\n",
        "         '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_qQb789_syt0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/q2_dagger_ant_Ant-v4_21-03-2023_19-00-28\n",
            "########################\n",
            "logging outputs to  data/q2_dagger_ant_Ant-v4_21-03-2023_19-00-28\n",
            "########################\n",
            "GPU not detected. Defaulting to CPU.\n",
            "Loading expert policy from... cs285/policies/experts/Ant.pkl\n",
            "obs (1, 111) (1, 111)\n",
            "Done restoring expert policy...\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Loading expert data for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sam/.virtualenvs/deep-rl/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/sam/.virtualenvs/deep-rl/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sam/.virtualenvs/deep-rl/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval_AverageReturn : 2871.16845703125\n",
            "Eval_StdReturn : 781.0054931640625\n",
            "Eval_MaxReturn : 3652.173828125\n",
            "Eval_MinReturn : 2090.162841796875\n",
            "Eval_AverageEpLen : 793.0\n",
            "Train_AverageReturn : 4713.6533203125\n",
            "Train_StdReturn : 12.196533203125\n",
            "Train_MaxReturn : 4725.849609375\n",
            "Train_MinReturn : 4701.45654296875\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 0\n",
            "TimeSinceStart : 31.09049105644226\n",
            "Training Loss : 0.031491827219724655\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4480.896484375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4480.896484375\n",
            "Eval_MinReturn : 4480.896484375\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 3697.1181640625\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 3697.1181640625\n",
            "Train_MinReturn : 3697.1181640625\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 1000\n",
            "TimeSinceStart : 43.97383689880371\n",
            "Training Loss : 0.010853566229343414\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4577.6591796875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4577.6591796875\n",
            "Eval_MinReturn : 4577.6591796875\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4518.62255859375\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4518.62255859375\n",
            "Train_MinReturn : 4518.62255859375\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 2000\n",
            "TimeSinceStart : 53.815216302871704\n",
            "Training Loss : 0.0049184635281562805\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4631.9794921875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4631.9794921875\n",
            "Eval_MinReturn : 4631.9794921875\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4769.31884765625\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4769.31884765625\n",
            "Train_MinReturn : 4769.31884765625\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 3000\n",
            "TimeSinceStart : 64.76776766777039\n",
            "Training Loss : 0.0027871280908584595\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4788.19189453125\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4788.19189453125\n",
            "Eval_MinReturn : 4788.19189453125\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4678.2158203125\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4678.2158203125\n",
            "Train_MinReturn : 4678.2158203125\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 4000\n",
            "TimeSinceStart : 76.77169561386108\n",
            "Training Loss : 0.001669079763814807\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Collecting train rollouts to be used for saving videos...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "\n",
            "Collecting video rollouts eval\n",
            "\n",
            "Saving train rollouts as videos...\n",
            "Eval_AverageReturn : 4674.9404296875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4674.9404296875\n",
            "Eval_MinReturn : 4674.9404296875\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4799.3544921875\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4799.3544921875\n",
            "Train_MinReturn : 4799.3544921875\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 5000\n",
            "TimeSinceStart : 89.67329549789429\n",
            "Training Loss : 0.0011206880444660783\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4856.47998046875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4856.47998046875\n",
            "Eval_MinReturn : 4856.47998046875\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4832.15380859375\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4832.15380859375\n",
            "Train_MinReturn : 4832.15380859375\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 6000\n",
            "TimeSinceStart : 100.90882778167725\n",
            "Training Loss : 0.0008353675366379321\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4695.30224609375\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4695.30224609375\n",
            "Eval_MinReturn : 4695.30224609375\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4749.0546875\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4749.0546875\n",
            "Train_MinReturn : 4749.0546875\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 7000\n",
            "TimeSinceStart : 113.2882604598999\n",
            "Training Loss : 0.0005810293951071799\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4732.51171875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4732.51171875\n",
            "Eval_MinReturn : 4732.51171875\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4668.22216796875\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4668.22216796875\n",
            "Train_MinReturn : 4668.22216796875\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 8000\n",
            "TimeSinceStart : 127.18264627456665\n",
            "Training Loss : 0.0004408238746691495\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Sampling new data to be used for training...\n",
            "\n",
            "Relabelling collected observations with labels from an expert policy...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4864.37841796875\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 4864.37841796875\n",
            "Eval_MinReturn : 4864.37841796875\n",
            "Eval_AverageEpLen : 1000.0\n",
            "Train_AverageReturn : 4767.96484375\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 4767.96484375\n",
            "Train_MinReturn : 4767.96484375\n",
            "Train_AverageEpLen : 1000.0\n",
            "Train_EnvstepsSoFar : 9000\n",
            "TimeSinceStart : 143.45276403427124\n",
            "Training Loss : 0.00045291008427739143\n",
            "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## run training\n",
        "print(args.logdir)\n",
        "trainer = BC_Trainer(args)\n",
        "trainer.run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "75M0MlR5tUIb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "TensorFlow installation not found - running with reduced feature set.\n",
              "\n",
              "NOTE: Using experimental fast data loading logic. To disable, pass\n",
              "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
              "    https://github.com/tensorflow/tensorboard/issues/4784\n",
              "\n",
              "I0321 19:02:52.883108 140676773664320 plugin.py:429] Monitor runs begin\n",
              "Address already in use\n",
              "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9onuUPfPEa"
      },
      "source": [
        "## Running DAgger (Problem 2)\n",
        "Modify the settings above:\n",
        "1. check the `do_dagger` box\n",
        "2. set `n_iters` to `10`\n",
        "3. set `exp_name` to `dagger_ant`\n",
        "and then rerun the code."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of run_hw1.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
